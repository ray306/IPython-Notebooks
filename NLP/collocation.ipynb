{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim import corpora, models, similarities\n",
    "from collections import Counter\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# read all documents in one variant\n",
    "path = 'f:\\\\nlp\\\\en\\\\COCA\\\\COCA_sample_lineartext\\\\'\n",
    "documents = []\n",
    "for filename in os.listdir(path+'result\\\\'):\n",
    "    with open(path+'result\\\\'+filename) as f:\n",
    "        documents += f.read().split('\\n')\n",
    "\n",
    "# remove common words and tokenize\n",
    "stopwords = set(stopwords.raw('english').split()) ## 获取英文停用词表\n",
    "texts = [[word for word in document.split() if word not in stopwords]\n",
    "         for document in documents]\n",
    "\n",
    "# remove words that appear only once\n",
    "all_tokens = [word for text in texts for word in text]\n",
    "freq = Counter(all_tokens)\n",
    "tokens_once = set(word for word in fd if fd[word] == 1)\n",
    "texts = [[word for word in text if word not in tokens_once]\n",
    "         for text in texts]\n",
    "\n",
    "# get th dictionary\n",
    "allTokens = set(word for word in fd if fd[word] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DictInit(keys,value):\n",
    "    Dict = {}\n",
    "    for k in keys:\n",
    "        Dict[k] = value\n",
    "    return Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f7dc85ea1210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFD_sent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFD_sent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mMI_sent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFD_sent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallTokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# conditional freq distribution and MI (using complete sentence)\n",
    "CFD_sent = DictInit(allTokens,Counter())\n",
    "MI_sent = DictInit(allTokens,{})\n",
    "\n",
    "for sent in texts:\n",
    "    for (index,token) in enumerate(sent):\n",
    "        CFD_sent[token].update(sent)\n",
    "\n",
    "wordsCount = sum([len(text) for text in texts])\n",
    "\n",
    "for t1 in CFD_sent:\n",
    "    for t2 in CFD_sent[t1]:\n",
    "        MI_sent[t1][t2] =math.log(CFD_sent[t1][t2]*len(allTokens)/(freq[t1]*freq[t2]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conditional freq distribution and MI (using complete sentence)\n",
    "CFD_sent = {}\n",
    "MI_sent ={}\n",
    "for word in allTokens:\n",
    "    CFD_sent[word] = Counter()\n",
    "    MI_sent[word] = {}\n",
    "    \n",
    "for sent in texts:\n",
    "    for (index,token) in enumerate(sent):\n",
    "        cfd_sent[token].update(sent)\n",
    "#         if index > 4:\n",
    "#             window = sent[index-5:index+5]   ## 前后5个单词,窗口\n",
    "#         else:\n",
    "#             window = sent[0:index+5]         ## 前后5个单词,窗口\n",
    "#         cfd_win[token].update(window)       ## 增加入当前单词的相关单词\n",
    "\n",
    "wordsCount = sum([len(text) for text in texts])\n",
    "for t1 in CFD_sent:\n",
    "    for t2 in CFD_sent[t1]:\n",
    "        pmi_sent[t1][t2] =math.log(float(CFD_sent[t1][t2])*len(allTokens)/(fd[t1]*fd[t2]),2)\n",
    "# for t1 in cfd_win:\n",
    "#     for t2 in cfd_win[t1]:\n",
    "#         cfd = float(cfd_win[t1][t2]+cfd_win[t2][t1])/2.0\n",
    "#         pmi_win[t1][t2] = cfd/len(tokens)*math.log(cfd*len(tokens)/(fd[t1]*fd[t2]),2)\n",
    "# for t1 in cfd_sent:\n",
    "#     for t2 in cfd_sent[t1]:\n",
    "#         pmi_sent[t1][t2] =math.log(float(cfd_sent[t1][t2])*len(tokens)/(fd[t1]*fd[t2]),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our corpora, Mutual Information is calculated as follows:\n",
    "\n",
    ">MI = log ( (AB * sizeCorpus) / (A * B * span) ) / log (2)\n",
    "\n",
    "Suppose we are calculating the MI for the collocate color near purple in BYU-BNC.\n",
    "\n",
    "A = frequency of node word (e.g. purple): 1262\n",
    "B = frequency of collocate (e.g. color): 115\n",
    "AB = frequency of collocate near the node word (e.g. color near purple): 24\n",
    "sizeCorpus= size of corpus (# words; in this case the BNC): 96,263,399\n",
    "span = span of words (e.g. 3 to left and 3 to right of node word): 6\n",
    "log (2) is literally the log10 of the number 2: .30103\n",
    "\n",
    ">MI = 11.37 = log ( (24 * 96,263,399) / (1262 * 115 * 6) ) / .30103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "path = 'D:\\\\nlp\\\\en\\\\COCA\\\\COCA_sample_lineartext\\\\'\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        if item not in ['has','was']:\n",
    "            stemmed.append(stemmer.lemmatize(item))\n",
    "    return ' '.join(stemmed)+'\\n'\n",
    "\n",
    "for filename in os.listdir(path+'raw'):\n",
    "    with open(path+'raw\\\\'+filename) as f:\n",
    "        doc = \"\"\n",
    "        for sent in f.read().split(' . '):\n",
    "            tokens = re.sub(r'<p>|(^| )[^a-z]+( |$)|[^a-z\\'\\.\\-]+',' ',sent.lower()).split()#去<p>，保留字母和词中的 ' - .\n",
    "            doc += stem_tokens(tokens,stemmer)\n",
    "        doc = doc[:-1]\n",
    "\n",
    "    with open(path+'result\\\\'+filename,'w') as f:\n",
    "        f.write(doc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "pmi_sent ={}\n",
    "for i in tokens:\n",
    "    pmi_sent[i] = {}\n",
    "\n",
    "wordsCount = sum([len(text) for text in texts])\n",
    "# for t1 in cfd_win:\n",
    "#     for t2 in cfd_win[t1]:\n",
    "#         cfd = float(cfd_win[t1][t2]+cfd_win[t2][t1])/2.0\n",
    "#         pmi_win[t1][t2] = cfd/len(tokens)*math.log(cfd*len(tokens)/(fd[t1]*fd[t2]),2)\n",
    "for t1 in cfd_sent:\n",
    "    for t2 in cfd_sent[t1]:\n",
    "        pmi_sent[t1][t2] =math.log(float(cfd_sent[t1][t2])*len(tokens)/(fd[t1]*fd[t2]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "pmi_sent_filtered = {}\n",
    "for i in pmi_sent:\n",
    "    pmi_sent_filtered[i] = [(k,v) for (k, v) in pmi_sent[i].items() if v > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target='penguin'\n",
    "\n",
    "re = Counter()\n",
    "for (token1,pmi1) in pmi_sent_filtered[target]:\n",
    "    for (token2,pmi2) in pmi_sent_filtered[token1]:\n",
    "        re.update({token2:pmi1*pmi2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.37415409088135\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "target='penguin'\n",
    "re = Counter()\n",
    "for (token1,pmi1) in pmi_sent_filtered[target]:\n",
    "    for (token2,pmi2) in pmi_sent_filtered[token1]:\n",
    "        tPMI = pmi1*pmi2\n",
    "        for (token3,pmi3) in pmi_sent_filtered[token2]:\n",
    "            re.update({token3:tPMI*pmi3})\n",
    "re.most_common(50)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.most_common(50)\n",
    "#re.most_common(len(filter(lambda v:v>50, re.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glass',\n",
       " 'golden',\n",
       " 'island',\n",
       " 'maestro',\n",
       " 'oil',\n",
       " 'photo',\n",
       " 'red',\n",
       " 'rock',\n",
       " 'sidebar',\n",
       " 'skin',\n",
       " 'small',\n",
       " 'surface',\n",
       " 'von'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(x[0] for x in thirdpmi)-set(x[0] for x in cfd_sent['cloud'].most_common(650))\n",
    "#set(thirdpmi)-set(re.viewkeys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(re,index=re.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BigramCollocationFinder' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-014116f53070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"I do not like green eggs and ham, I do not \\\"like\\\" them Sam I am!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBigramCollocationFinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'12121234'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mh:\\Anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m         '''\n\u001b[0;32m    461\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    540\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'BigramCollocationFinder' object is not iterable"
     ]
    }
   ],
   "source": [
    "text = \"I do not like green eggs and ham, I do not \\\"like\\\" them Sam I am!\"\n",
    "tokens = nltk.wordpunct_tokenize(text)\n",
    "print(nltk.FreqDist(BigramCollocationFinder.from_words('12121234',window_size = 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775185352641972"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.spearman import *\n",
    "spearman_correlation(pmi_sent['se'],pmi_sent['handbook'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', '4'), ('2', '3'), ('1', '2'), ('4', '1'), ('2', '1')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words('121212341')\n",
    "print finder.nbest(bigram_measures.pmi, 10)  \n",
    "finder.apply_freq_filter(3)\n",
    "print finder.nbest(bigram_measures.pmi, 10)\n",
    "finder = BigramCollocationFinder.from_words('12121234',window_size = 3)\n",
    "print finder.nbest(bigram_measures.pmi, 10)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
