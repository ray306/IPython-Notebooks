{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'用sun拼音的4m词库确定词频(是目前唯一有词频的txt词库)'\n",
    "path = 'D:\\\\nlp\\\\zh\\\\sun pinyin dict.txt'\n",
    "# words = pd.read_csv(path,header=None,sep=' ')\n",
    "words = []\n",
    "with open(path,encoding='utf-8') as f:\n",
    "    for l in f:\n",
    "        t = l[:-1].split(' ')\n",
    "        words.append((t[0],int(t[-1])))\n",
    "words = pd.DataFrame(words)\n",
    "word4c = words[words[0].str.len()==4]\n",
    "#找所有到双字词\n",
    "word2c = words[words[0].str.len()==2]\n",
    "#并转成set\n",
    "word2c_set = set(word2c[0].tolist())\n",
    "#freq dict\n",
    "word2c_dict = dict((i[0],i[1]) for ind,i in word2c.iterrows())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word4c_dict = dict((i[0],i[1]) for ind,i in word4c.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'用jieba的8M大词库确定词性(但这个词库连\"希腊神话\"都没有,所以只用它判断双字词的词性)'\n",
    "path2 = 'D:\\\\nlp\\\\zh\\\\dict.txt.big'\n",
    "#打开jieba的词表(eg. 滿臉堆笑 3 vn)\n",
    "word_att = pd.read_csv(path2,header=None,sep=' ')\n",
    "#找所有到词性含有Noun的词\n",
    "noun = word_att[(1-pd.isnull(word_att[2])) & (word_att[2].str.contains('n'))]\n",
    "#找所有到双字名词\n",
    "noun2c = noun[noun[0].str.len()==2]\n",
    "#并转成set\n",
    "noun2c_set = set(noun2c[0].tolist())\n",
    "#freq dict\n",
    "noun2c_dict = dict((i[0],i[1]) for ind,i in noun2c.iterrows())\n",
    "#找所有到双字词\n",
    "word2c = word_att[word_att[0].str.len()==2]\n",
    "#并转成set\n",
    "word2c_set = set(word2c[0].tolist())\n",
    "#freq dict\n",
    "word2c_dict = dict((i[0],i[1]) for ind,i in word2c.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'找到sun拼音词库中有没有成分是名词的四字词'\n",
    "result = []\n",
    "for idx,i in word4c.iterrows():\n",
    "    if i[0][0:2] not in word2c_set and i[0][1:3] not in word2c_set and i[0][2:4] not in word2c_set:\n",
    "        try:\n",
    "            result.append((i[0],i[1]))\n",
    "        except:\n",
    "            pass\n",
    "word4c_0n = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'找到sun拼音词库中有一个成分是名词的四字词'\n",
    "result = []\n",
    "for idx,i in word4c.iterrows():\n",
    "    if (i[0][0:2] in noun2c_set and i[0][2:4] not in word2c_set) \n",
    "        or (i[0][2:4] in noun2c_set and i[0][0:2] not in word2c_set):\n",
    "        try:\n",
    "            result.append((i[0],i[1],i[0][0:2],word2c_dict[i[0][0:2]],i[0][2:4]))\n",
    "        except:\n",
    "            pass\n",
    "word4c_1n = pd.DataFrame(result)\n",
    "word4c_1n[(word4c_1n[1]>2000) & (result[3]>20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'找到sun拼音词库中所有成分都是名词的四字词'\n",
    "result = []\n",
    "for idx,i in word4c.iterrows():\n",
    "    if i[0][0:2] in word2c_set and i[0][2:4] in word2c_set:\n",
    "        try:\n",
    "            result.append((i[0],i[1],i[0][0:2],word2c_dict[i[0][0:2]],i[0][2:4],word2c_dict[i[0][2:4]]))\n",
    "        except:\n",
    "            pass\n",
    "word4c_2n = pd.DataFrame(result)\n",
    "word4c_2n.to_csv('D:\\\\nlp\\\\zh\\\\sun_bigram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'生成由随机双字名词组合的(sun拼音词库没有的)四字词'\n",
    "import random\n",
    "result = []\n",
    "word4c_set = set(word4c[0].tolist())\n",
    "\n",
    "def isJian(x):\n",
    "    try:\n",
    "        x.encode('gb2312')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "noun2c_jian = noun2c[noun2c[0].map(lambda x:isJian(x))] # 只保留简体词\n",
    "noun2c_jian_list = list(noun2c_jian.sort_index(by=1,ascending=False)[:1500][0])\n",
    "random.shuffle(noun2c_jian_list)\n",
    "\n",
    "for i in range(500):\n",
    "    w1 = noun2c_jian_list[i*2]\n",
    "    w2 = noun2c_jian_list[i*2+1]\n",
    "    if w1!=w2 and (w1+w2 not in word4c_set):\n",
    "        result.append((w1+w2))\n",
    "nonword4c_2n = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stroke_data = nonword4c_2n\n",
    "stroke_data['stroke1'] = stroke_data[0].map(lambda x:sql_result[x[0]])\n",
    "stroke_data['stroke2'] = stroke_data[0].map(lambda x:sql_result[x[1]])\n",
    "stroke_data['stroke3'] = stroke_data[0].map(lambda x:sql_result[x[2]])\n",
    "stroke_data['stroke4'] = stroke_data[0].map(lambda x:sql_result[x[3]])\n",
    "stroke_data['stroke12'] = stroke_data[0].map(lambda x:sql_result[x[0]]+sql_result[x[1]])\n",
    "stroke_data['stroke34'] = stroke_data[0].map(lambda x:sql_result[x[2]]+sql_result[x[3]])\n",
    "stroke_data\n",
    "# 4个字的笔画都要在某范围内\n",
    "condition1 = stroke_data.ix[:,['stroke1','stroke2','stroke3','stroke4']].apply(lambda x:3<min(x) and max(x)<14, axis=1)\n",
    "# 2个词的笔画都要在某范围内\n",
    "condition2 = stroke_data.ix[:,['stroke12','stroke34']].apply(lambda x:9<min(x) and max(x)<25, axis=1)\n",
    "# 没有重复的字\n",
    "condition3 = stroke_data.ix[:,0].apply(lambda x:len(set(x))==4)\n",
    "\n",
    "stroke_data[condition1 & condition2 & condition3].ix[:,0].to_csv('D:\\\\nlp\\\\zh\\\\nonword_2n.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect('h:\\\\lab\\\\hanzi.db')\n",
    "# import pandas.io.sql as sql\n",
    "# t=sql.read_frame('SELECT Character,Stroke_Num FROM hanzi',con)\n",
    "\n",
    "c = con.cursor()\n",
    "c.execute('SELECT Character,Stroke_Num FROM hanzi')\n",
    "sql_result = dict(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'增加笔画数的列'\n",
    "stroke_data = word4c_0n[word4c_0n[1]>3000].sort_index(by=1,ascending=False)\n",
    "stroke_data['stroke1'] = stroke_data[0].map(lambda x:sql_result[x[0]])\n",
    "stroke_data['stroke2'] = stroke_data[0].map(lambda x:sql_result[x[1]])\n",
    "stroke_data['stroke3'] = stroke_data[0].map(lambda x:sql_result[x[2]])\n",
    "stroke_data['stroke4'] = stroke_data[0].map(lambda x:sql_result[x[3]])\n",
    "stroke_data['stroke12'] = stroke_data[0].map(lambda x:sql_result[x[0]]+sql_result[x[1]])\n",
    "stroke_data['stroke34'] = stroke_data[0].map(lambda x:sql_result[x[2]]+sql_result[x[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'按笔画数筛选'\n",
    "# 4个字的笔画都要在某范围内\n",
    "condition1 = stroke_data.ix[:,['stroke1','stroke2','stroke3','stroke4']].apply(lambda x:3<min(x) and max(x)<14, axis=1)\n",
    "# 2个词的笔画都要在某范围内\n",
    "condition2 = stroke_data.ix[:,['stroke12','stroke34']].apply(lambda x:9<min(x) and max(x)<25, axis=1)\n",
    "# 没有重复的字\n",
    "condition3 = stroke_data.ix[:,0].apply(lambda x:len(set(x))==4)\n",
    "\n",
    "stroke_data[condition1 & condition2 & condition3].ix[:,:2].to_csv('D:\\\\nlp\\\\zh\\\\sun_bigram_0n.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stroke_filter_shuffle = stroke_filter.ix[:,0].reindex(random.permutation(stroke_filter.index))\n",
    "# stroke_filter_shuffle.column = ['word','frequence','wholeness','transparent']\n",
    "stroke_filter_shuffle.to_csv('D:\\\\nlp\\\\zh\\\\test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path3 = 'D:\\\\nlp\\\\zh\\\\sun_bigram.xlsx'\n",
    "rawdata = pd.ExcelFile(path3).parse('Sheet2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'orthographic neighborhood rating'\n",
    "word = '一无所求'\n",
    "\n",
    "w12 = len(word4c[word4c[0].map(lambda x: True if re.match('('+word[:2]+'..)',x) else False)])\n",
    "w34 = len(word4c[word4c[0].map(lambda x: True if re.match('(..'+word[2:]+')',x) else False)])\n",
    "\n",
    "w1 = len(word4c[word4c[0].map(lambda x: True if re.match('(.'+word[1:]+')',x) else False)])\n",
    "w2 = len(word4c[word4c[0].map(lambda x: True if re.match('('+word[:1]+'.'+word[2:]+')',x) else False)])\n",
    "w3 = len(word4c[word4c[0].map(lambda x: True if re.match('('+word[:2]+'.'+word[3:]+')',x) else False)])\n",
    "w4 = len(word4c[word4c[0].map(lambda x: True if re.match('('+word[:3]+'.)',x) else False)])\n",
    "w12,w34,w1,w2,w3,w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path3 = 'D:\\\\nlp\\\\zh\\\\test.csv'\n",
    "rawdata = pd.read_csv(path3,encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'按和真词的词素log-freq的匹配去筛选伪词'\n",
    "def dic(x):\n",
    "    try:\n",
    "        return math.log(word2c_dict[x])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "rawdata = nonword4c_2n\n",
    "\n",
    "rawdata['part1_logf'] = rawdata[0].map(lambda x:dic(x[:2]))\n",
    "rawdata['part2_logf'] = rawdata[0].map(lambda x:dic(x[2:]))\n",
    "\n",
    "condition2 = rawdata['part1_logf'].map(lambda x: 14.5521083058 < x < 17.0795804504)\n",
    "condition3 = rawdata['part2_logf'].map(lambda x: 14.7174585192 < x < 17.0045341867)\n",
    "sum(condition2 & condition3)/len(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'按log-freq在2个sd内去筛选整词和词素'\n",
    "import math\n",
    "rawdata['w_logf'] = rawdata['w_f'].map(lambda x:math.log(x))\n",
    "rawdata['part1_logf'] = rawdata['part1_f'].map(lambda x:math.log(x))\n",
    "rawdata['part2_logf'] = rawdata['part2_f'].map(lambda x:math.log(x))\n",
    "\n",
    "def getAvailableWordFreq(data):\n",
    "    mid = mean(data)\n",
    "    bias = 2*std(data)\n",
    "    print(mid-bias,mid+bias)\n",
    "    return data.map(lambda x: mid-bias < x < mid+bias)\n",
    "\n",
    "condition1 = getAvailableWordFreq(rawdata['w_logf'])\n",
    "condition2 = getAvailableWordFreq(rawdata['part1_logf'])\n",
    "condition3 = getAvailableWordFreq(rawdata['part2_logf'])\n",
    "freq_filter = rawdata[condition1 & condition2 & condition3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'cai的数据库中提取字的信息'\n",
    "char = pd.ExcelFile('D:\\\\SUBTLEX-CH-CHR.xlsx').parse('SUBTLEX-CH-CHR')\n",
    "char_freq = dict([(i.Character,i.logCHR) for idx,i in char.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars_avail = [i.Character for idx,i in char.iterrows() if 3.01129056296<i.logCHR<5.34449134312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9436070833333332"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pseudoword = []\n",
    "for i in range(300):\n",
    "    pseudoword.append(''.join([random.choice(chars_avail) for i in range(4)]))\n",
    "stroke_data = pd.DataFrame(pseudoword)\n",
    "stroke_data['stroke1'] = stroke_data[0].map(lambda x:sql_result[x[0]])\n",
    "stroke_data['stroke2'] = stroke_data[0].map(lambda x:sql_result[x[1]])\n",
    "stroke_data['stroke3'] = stroke_data[0].map(lambda x:sql_result[x[2]])\n",
    "stroke_data['stroke4'] = stroke_data[0].map(lambda x:sql_result[x[3]])\n",
    "stroke_data['stroke12'] = stroke_data[0].map(lambda x:sql_result[x[0]]+sql_result[x[1]])\n",
    "stroke_data['stroke34'] = stroke_data[0].map(lambda x:sql_result[x[2]]+sql_result[x[3]])\n",
    "\n",
    "# 4个字的笔画都要在某范围内\n",
    "condition1 = stroke_data.ix[:,['stroke1','stroke2','stroke3','stroke4']].apply(lambda x:3<min(x) and max(x)<14, axis=1)\n",
    "# 2个词的笔画都要在某范围内\n",
    "condition2 = stroke_data.ix[:,['stroke12','stroke34']].apply(lambda x:9<min(x) and max(x)<25, axis=1)\n",
    "# 没有重复的字\n",
    "condition3 = stroke_data.ix[:,0].apply(lambda x:len(set(x))==4)\n",
    "\n",
    "import random\n",
    "F_ff = stroke_data[condition1 & condition2 & condition3]\n",
    "data = F_ff.ix[random.sample(F_ff.index.tolist(), 120),:]\n",
    "\n",
    "char1_freq = data[0].map(lambda x:char_freq[x[0]],2).tolist()\n",
    "char2_freq = data[0].map(lambda x:char_freq[x[1]],2).tolist()\n",
    "char3_freq = data[0].map(lambda x:char_freq[x[2]],2).tolist()\n",
    "char4_freq = data[0].map(lambda x:char_freq[x[3]],2).tolist()\n",
    "mean([char1_freq+char2_freq+char3_freq+char4_freq])\n",
    "\n",
    "\n",
    "# stroke_data[condition1 & condition2 & condition3].ix[:,0].to_csv('D:\\\\nlp\\\\zh\\\\nonword_0n.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "'match'\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "path = 'D:\\\\nlp\\\\zh\\\\stimuli\\\\match\\\\'\n",
    "T_tt = pd.read_csv(path+'T_tt.csv',encoding='gbk')\n",
    "T_tt = T_tt.reindex(np.random.permutation(T_tt.index))\n",
    "T_tt.index = list(range(len(T_tt)))\n",
    "# T_tt1 = T_tt.ix[random.sample(T_tt.index.tolist(), 120),:]\n",
    "# .copy(deep=True)\n",
    "T_tt1 = T_tt.ix[:119,]\n",
    "\n",
    "\n",
    "T_tt1['WholeAcc'] = 1\n",
    "T_tt1['LeftAcc'] = 1\n",
    "T_tt1['RightAcc'] = 1\n",
    "T_tt1.to_csv(path+'T_tt1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'把正常的四字词的两个词素拿出来打乱重排'\n",
    "F_tt = T_tt.ix[120:239,]\n",
    "parts = F_tt['Word'].map(lambda x:x[:2]).tolist() + F_tt['Word'].map(lambda x:x[2:]).tolist()\n",
    "random.shuffle(parts)\n",
    "\n",
    "F_tt1 = []\n",
    "for i in zip(parts[:len(parts)//2],parts[len(parts)//2:]):\n",
    "    word = i[0]+i[1]\n",
    "    if word not in word4c:\n",
    "        F_tt1.append(word)\n",
    "        \n",
    "pd.DataFrame(F_tt1,columns=['Word']).to_csv(path+'F_tt1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'把redo标记为1的词拿出来打乱重排, 加到文件末尾'\n",
    "F_tt = pd.read_csv(path+'F_tt1.csv',encoding='gbk')\n",
    "redo = F_tt[F_tt.redo==1]\n",
    "parts = redo['Word'].map(lambda x:x[:2]).tolist() + redo['Word'].map(lambda x:x[2:]).tolist()\n",
    "random.shuffle(parts)\n",
    "\n",
    "F_tt1 = []\n",
    "for i in zip(parts[:len(parts)//2],parts[len(parts)//2:]):\n",
    "    word = i[0]+i[1]\n",
    "    if word not in word4c:\n",
    "        F_tt1.append(word)\n",
    "        \n",
    "F_tt[F_tt.redo.isnull()].append(pd.DataFrame(F_tt1,columns=['Word'])).to_csv(path+'F_tt1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'把正常的四字词的四个字拿出来打乱重排'\n",
    "F_ff = T_tt.ix[240:359,]\n",
    "parts = []\n",
    "for i in F_ff['Word']:\n",
    "    for idx in range(4):\n",
    "        parts.append(i[idx])\n",
    "\n",
    "random.shuffle(parts)\n",
    "\n",
    "parts = np.array(parts)\n",
    "F_ff1 = [''.join(i) for i in parts.reshape(120,4)]\n",
    "\n",
    "pd.DataFrame(F_ff1,columns=['Word']).to_csv(path+'F_ff1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'把redo标记为1的词拿出来打乱重排, 加到文件末尾'\n",
    "F_ff = pd.read_csv(path+'F_ff1.csv',encoding='gbk')\n",
    "redo = F_ff[F_ff.redo==1]['Word']\n",
    "parts = []\n",
    "for i in redo:\n",
    "    for idx in range(4):\n",
    "        parts.append(i[idx])\n",
    "        \n",
    "random.shuffle(parts)\n",
    "\n",
    "parts = np.array(parts)\n",
    "F_ff1 = [''.join(i) for i in parts.reshape(len(parts)//4,4)]\n",
    "        \n",
    "F_ff[F_ff.redo.isnull()].append(pd.DataFrame(F_ff1,columns=['Word'])).to_csv(path+'F_ff1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T_tt1 = pd.read_csv(path+'T_tt1.csv',encoding='gbk')\n",
    "T_ff1 = pd.read_csv(path+'T_ff1.csv',encoding='gbk')\n",
    "F_tt1 = pd.read_csv(path+'F_tt1.csv',encoding='gbk')\n",
    "F_ff1 = pd.read_csv(path+'F_ff1.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2070939716312052, 0.87454483579849396)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = T_ff1.copy(deep=True)\n",
    "char1_freq = data.Word.map(lambda x:char_freq[x[0]]).tolist()\n",
    "char2_freq = data.Word.map(lambda x:char_freq[x[1]]).tolist()\n",
    "char3_freq = data.Word.map(lambda x:char_freq[x[2]]).tolist()\n",
    "char4_freq = data.Word.map(lambda x:char_freq[x[3]]).tolist()\n",
    "\n",
    "mean([char1_freq+char2_freq+char3_freq+char4_freq]),std([char1_freq+char2_freq+char3_freq+char4_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8427616.66667 2748430.97871\n",
      "8381758.33333 2882498.87828\n",
      "8473475.0 2606672.1951\n"
     ]
    }
   ],
   "source": [
    "data = T_tt1.copy(deep=True)\n",
    "part1_freq = data.Word.map(lambda x:word2c_dict[x[:2]]).tolist()\n",
    "part2_freq = data.Word.map(lambda x:word2c_dict[x[2:]]).tolist()\n",
    "print(mean([part1_freq+part2_freq]),std([part1_freq+part2_freq]))\n",
    "print(mean(part1_freq),std(part1_freq))\n",
    "print(mean(part2_freq),std(part2_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575917.355372 333205.855186\n"
     ]
    }
   ],
   "source": [
    "data = T_ff1.copy(deep=True)\n",
    "freq = data.ix[:120,].Word.map(lambda x:word4c_dict[x]).tolist()\n",
    "\n",
    "print(mean(freq),std(freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16615466.666666666"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_csv('D:\\\\nlp\\\\zh\\\\test.csv',encoding='gbk')\n",
    "data = rawdata.ix[random.sample(rawdata.index.tolist(), 120),:]\n",
    "mean(data.part1_f+data.part2_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1835416666666667"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = T_tt.ix[random.sample(T_tt.index.tolist(), 60),:]\n",
    "char1_freq = data.Word.map(lambda x:round(char_freq[x[0]],2)).tolist()\n",
    "char2_freq = data.Word.map(lambda x:round(char_freq[x[1]],2)).tolist()\n",
    "char3_freq = data.Word.map(lambda x:round(char_freq[x[2]],2)).tolist()\n",
    "char4_freq = data.Word.map(lambda x:round(char_freq[x[3]],2)).tolist()\n",
    "\n",
    "mean([char1_freq+char2_freq+char3_freq+char4_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAvailableCharFreq(data):\n",
    "    mid = mean(data)\n",
    "    bias = 2*std(data)\n",
    "    return mid-bias,mid+bias\n",
    "freqRange = getAvailableCharFreq([char1_freq+char2_freq+char3_freq+char4_freq])\n",
    "chars_avail = [i.Character for idx,i in char.iterrows() if freqRange[0]<i.logCHR<freqRange[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F_ff = pd.read_csv(path+'F_ff.csv',encoding='gbk')\n",
    "F_ff1 = stroke_data\n",
    "F_ff1['c1_f'] = F_ff1.Word.map(lambda x: char_freq[x[0]]).tolist()\n",
    "F_ff1['c2_f'] = F_ff1.Word.map(lambda x: char_freq[x[1]]).tolist()\n",
    "F_ff1['c3_f'] = F_ff1.Word.map(lambda x: char_freq[x[2]]).tolist()\n",
    "F_ff1['c4_f'] = F_ff1.Word.map(lambda x: char_freq[x[3]]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "data = F_ff.ix[random.sample(F_ff.index.tolist(), 120),:]\n",
    "char1_freq = data.Word.map(lambda x:round(char_freq[x[0]],2)).tolist()\n",
    "char2_freq = data.Word.map(lambda x:round(char_freq[x[1]],2)).tolist()\n",
    "char3_freq = data.Word.map(lambda x:round(char_freq[x[2]],2)).tolist()\n",
    "char4_freq = data.Word.map(lambda x:round(char_freq[x[3]],2)).tolist()\n",
    "\n",
    "char_freq_fff = [char1_freq+char2_freq+char3_freq+char4_freq]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
