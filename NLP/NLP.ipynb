{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "cfd = {}\n",
    "path = 'd:\\\\nlp\\\\brown\\\\'\n",
    "\n",
    "with open('d:\\\\nlp\\\\brown_stats\\\\idf.txt') as f2:\n",
    "    idf = pickle.load(f2)\n",
    "    for i in idf:\n",
    "        cfd[i]=Counter()\n",
    "        \n",
    "for filename in os.listdir(path):\n",
    "    with open(path+filename) as f1:            \n",
    "        sentences = f1.read().split('\\n')\n",
    "        for sent in sentences:\n",
    "            wordseq = sent.split(' ')\n",
    "            for (index,token) in enumerate(wordseq):\n",
    "                if index > 4:\n",
    "                    window = wordseq[index-5:index+5]   ## 前后5个单词,窗口\n",
    "                else:\n",
    "                    window = wordseq[0:index+5]         ## 前后5个单词,窗口\n",
    "                for subtoken in window:\n",
    "                    try:\n",
    "                        cfd[token].update({subtoken:idf[subtoken]})       ## 增加入当前单词的相关单词\n",
    "                    except:\n",
    "                        pass\n",
    "print 'fin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "cfd = {}\n",
    "\n",
    "for i in idf:\n",
    "    cfd[i]=Counter()\n",
    "\n",
    "for sent in docs:\n",
    "    wordseq = sent.split(' ')\n",
    "    for (index,token) in enumerate(wordseq):\n",
    "        if index > 4:\n",
    "            window = wordseq[index-5:index+5]   ## 前后5个单词,窗口\n",
    "        else:\n",
    "            window = wordseq[0:index+5]         ## 前后5个单词,窗口\n",
    "        for subtoken in window:\n",
    "            try:\n",
    "                cfd[token].update({subtoken:idf[subtoken]})       ## 增加入当前单词的相关单词\n",
    "            except:\n",
    "                pass\n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.word_fd['3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import os\n",
    "\n",
    "path = 'D:\\\\nlp\\\\subtitle\\\\'\n",
    "\n",
    "with open(path+'Big Srt_PoS.txt',encoding='utf-8') as f:\n",
    "    docs = ['']\n",
    "    now = []\n",
    "    for sent in f:\n",
    "        if sent == '\\n':\n",
    "            docs[-1] = ' '.join(now)\n",
    "            now = []\n",
    "            docs.append(\"\")\n",
    "        if 'Subtitle:' in sent:\n",
    "            continue\n",
    "        now.extend(re.findall(r'([(\\u4e00-\\u9fa5)]+)',sent))\n",
    "\n",
    "with open(path+'cleaned','w') as f:\n",
    "    f.write('\\n'.join(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-23b2151c5bf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0midf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not bytes"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "import pickle\n",
    "path = 'D:\\\\nlp\\\\course\\\\'\n",
    "\n",
    "alldoc = Counter() #有某词出现的文档量\n",
    "\n",
    "for doc in docs:\n",
    "    wordSet = set(doc.split(' '))\n",
    "    alldoc.update(wordSet)\n",
    "    \n",
    "#计算idf\n",
    "with open(path+'idf.txt','w') as f:\n",
    "    idf = dict([(i,alldoc[i]) for i in alldoc])\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(docs)/idf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005957998182729808"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['你']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe4\\xb8\\x80\\xe4\\xb8\\xaa/  \\xe4\\xbb\\x96\\xe4\\xbb\\xac/  \\xe5\\xba\\x94\\xe5\\xbe\\x97/v \\xe7\\x9a\\x84/  \\xe6\\x94\\xbf\\xe5\\xba\\x9c/  \\xe2\\x80\\x9d/w '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = '一个/mq 他们/r 应得/v 的/u 政府/n ”/w '\n",
    "\n",
    "re.sub(r'([(\\u4e00-\\u9fa5)]+)',' ',line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "path = 'D:\\\\nlp\\\\course\\\\'\n",
    "\n",
    "alldoc = Counter() #有某词出现的文档量\n",
    "\n",
    "with open(path+'cleaned') as f:\n",
    "    docs = f.read().split('\\n')\n",
    "    for doc in docs:\n",
    "        wordSet = set(doc.split(' '))\n",
    "        alldoc.update(wordSet)\n",
    "    \n",
    "#计算idf\n",
    "with open(path+'idf.txt','w') as f:\n",
    "    idf = dict([(i,alldoc[i]) for i in alldoc])\n",
    "    for i in idf:\n",
    "        idf[i] = math.log(len(docs)/idf[i])\n",
    "    pickle.dump(idf, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-4dccc18b4b16>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-4dccc18b4b16>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    c d\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n = 'change'\n",
    "c=Counter()\n",
    "for e1 in cooccur[n][-30:].keys():\n",
    "    c.update(dict(cooccur[e1][-30:]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=[Counter() for i in range(4)]\n",
    "obj = \"road\"\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    c[0].update(cfd[e1].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        c[1].update(cfd[e2].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        for e3 in cfd[e2].keys()[:10]:\n",
    "            c[2].update(cfd[e3].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        for e3 in cfd[e2].keys()[:10]:\n",
    "            for e4 in cfd[e3].keys()[:10]:\n",
    "                c[3].update(cfd[e4].keys()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c='1,2,352'\n",
    "c.count(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread\n",
      "life\n",
      "road\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "fd = FreqDist()\n",
    "cfd = ConditionalFreqDist()\n",
    "stopwords_list = list(stopwords.raw('english')) ## 获取英文停用词表\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NN$', 'NN-TL', 'NN+BEZ', 'NN+HVZ', 'NNS$', 'NP', 'NP$', 'NP+BEZ', 'NPS', 'NPS$', 'NR', 'NRS', 'NR$']\n",
    "def not_punctuation(tag):\n",
    "    return tag not in ['.', '--', '(', ')', ',', ':']\n",
    "\n",
    "for sentence in brown.tagged_sents():                  ## 外层for运行时间很长\n",
    "    for (index, tagtuple) in enumerate(sentence):\n",
    "        (token, tag) = tagtuple\n",
    "        token = token.lower()\n",
    "        if token not in stopwords_list and not_punctuation(tag):\n",
    "            if index > 4:\n",
    "                window = sentence[index-5:index+5]   ## 前后5个单词,窗口\n",
    "            else:\n",
    "                window = sentence[0:index+5]         ## 前后5个单词,窗口\n",
    "            for (window_token, window_tag) in window:\n",
    "                window_token = window_token.lower()\n",
    "                if window_token not in stopwords_list and not_punctuation(tag):\n",
    "                    cfd[token].inc(window_token)       ## 增加入当前单词的相关单词\n",
    "\n",
    "print cfd['bread'].max()  ## cheese\n",
    "print cfd['life'].max()   ## death\n",
    "print cfd['road'].max()   ## block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords_list =   ## 获取英文停用词表\n",
    "\n",
    "def is_punctuation(token):\n",
    "    return re.match(r'[^a-z]+',token)\n",
    "\n",
    "for sentence in brown.sents():                  ## 外层for运行时间很长\n",
    "    for (index, token) in enumerate(sentence):\n",
    "        token = token.lower()\n",
    "        if token not in stopwords_list and not is_punctuation(token):\n",
    "            if index > 4:\n",
    "                window = sentence[index-5:index+5]   ## 前后5个单词,窗口\n",
    "            else:\n",
    "                window = sentence[0:index+5]         ## 前后5个单词,窗口\n",
    "            for window_token in window:\n",
    "                window_token = window_token.lower()\n",
    "                if window_token not in stopwords_list and not is_punctuation(token):\n",
    "                    cfd[token].inc(window_token)       ## 增加入当前单词的相关单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c=[Counter() for i in range(4)]\n",
    "obj = \"road\"\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    c[0].update(cfd[e1].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        c[1].update(cfd[e2].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        for e3 in cfd[e2].keys()[:10]:\n",
    "            c[2].update(cfd[e3].keys()[:10])\n",
    "for e1 in cfd[obj].keys()[:10]:\n",
    "    for e2 in cfd[e1].keys()[:10]:\n",
    "        for e3 in cfd[e2].keys()[:10]:\n",
    "            for e4 in cfd[e3].keys()[:10]:\n",
    "                c[3].update(cfd[e4].keys()[:10])\n",
    "# for e1 in cfd[obj].keys()[:10]:\n",
    "#     for e2 in cfd[e1].keys()[:10]:\n",
    "#         for e3 in cfd[e2].keys()[:10]:\n",
    "#             for e4 in cfd[e3].keys()[:10]:\n",
    "#                 for e5 in cfd[e4].keys()[:10]:\n",
    "#                     c[5].update(cfd[e5].keys()[:10])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "re = ['']*4\n",
    "for i in range(4):\n",
    "    re[i] = pd.Series(c[i]).order()[:-25:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'2': 3.1, '3': 1})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c=Counter()\n",
    "c.update('223')\n",
    "c['2']+=1.1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-b72487219bee>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-b72487219bee>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    prin t set([(i) for i in c if c[i]>1]) - set([(i) for i in cfd[\"road\"] if cfd[\"road\"][i]>1])\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#print set([(i) for i in cfd[\"road\"]])\n",
    "prin t set([(i) for i in c if c[i]>1]) - set([(i) for i in cfd[\"road\"] if cfd[\"road\"][i]>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn\n",
      "mid-week\n",
      "deferment\n",
      "woods\n",
      "bilharziasis\n",
      "hanging\n",
      "francesca\n",
      "todman\n",
      "sprague\n",
      "orthographies\n",
      "kurigalzu\n",
      "$25-a-plate\n",
      "gab\n",
      "originality\n",
      "diplomat's\n",
      "wrought-iron\n",
      "familiarness\n",
      "co-operation\n",
      "franklin's\n",
      "pigment\n"
     ]
    }
   ],
   "source": [
    "for i,y in zip(cfd,range(20)):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Displaying 25 of 724 matches:\n",
      "                                      我 住在 同一條 巷子 我們 是 �\n",
      "�� 一起 回家 有一天 上學 時 我 到 她 家 等候 按 了 門鈴 卻\n",
      "��鈴 卻 沒有 任何 動靜 正當 我 想 離開 時 門 內 突然 傳來 \n",
      "�� 了 門 大聲 的 叫 著 快 點 我 媽媽 暈倒 了 嘉珍 抓起 我 �\n",
      "� 我 媽媽 暈倒 了 嘉珍 抓起 我 的 手 急忙 往 屋 裡 跑 進入\n",
      " 得 像 紙 一樣 這種 情景 把 我 嚇壞 了 怎麼辦 嘉珍 不停 �\n",
      "��停 的 哭泣 聲音 有些 顫抖 我 的 腦海 中 頓時 一片 空白 �\n",
      "� 怎麼辦 才 好 過 了 一會兒 我 才 問 她 你 爸爸 呢 他 出差\n",
      " 他 出差 了 嘉珍 擦 著 眼淚 我 握住 她 的 雙手 她 的 手 又\n",
      "�� 這時 有個 念頭 突然 閃過 我 的 眼前 我 幫 她 撥 了 一一\n",
      "� 念頭 突然 閃過 我 的 眼前 我 幫 她 撥 了 一一九 請 救護�\n",
      " 她 的 背 安慰 她 不要 著急 我 會 陪 你 的 不久 救護車 停 \n",
      "��上 救護車 嘉珍 上車 前 對 我 說 謝謝 你 的 幫忙 我 握 著\n",
      "前 對 我 說 謝謝 你 的 幫忙 我 握 著 她 的 手 說 不用 謝 �\n",
      "�� 握 著 她 的 手 說 不用 謝 我 因為 我們 是 好朋友 二 無�\n",
      "員外 來 找 他 並且 對 他 說 我 看 你 悶悶不樂 是不是 進京\n",
      "�� 著 他 的 手 說 你 不用 還 我 了 我 只是 盡 一份 心力 而�\n",
      "�� 的 手 說 你 不用 還 我 了 我 只是 盡 一份 心力 而已 以�\n",
      " 銀子 含 著 眼淚 說 謝謝 您 我 不 知道 要 怎麼 來 報答 您 \n",
      "答 您 江 巡撫 說 你 不必 謝 我 也 不必 回報 我 四 快樂 的 \n",
      " 你 不必 謝 我 也 不必 回報 我 四 快樂 的 閱讀課 上 國語�\n",
      "�� 這一組 做 讀書 心得 報告 我 第一個 站起來 發言 我們 閱\n",
      "�過 兩 人 成為 知心 的 朋友 我 說完 之後 組長 站起來 補充\n",
      "清理 火山灰 而且 欣賞 落日 我 喜歡 這種 自由自在 的 生活\n",
      "的 人 也 不 喜歡 虛偽 的 人 我 覺得 小 王子 很 正直 最後 �\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sinica_treebank\n",
    " \n",
    "sinica_text = nltk.Text(sinica_treebank.words())\n",
    "print sinica_text.concordance('我')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
