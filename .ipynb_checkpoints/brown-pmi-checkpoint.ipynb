{
 "metadata": {
  "name": "",
  "signature": "sha256:a99a657460456c9f50a75dc8382c92be6567190c9647529a8d7f57503478c253"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "import re\n",
      "import os\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "stemmer = WordNetLemmatizer()\n",
      "\n",
      "path = 'd:\\\\nlp\\\\brown\\\\'\n",
      "\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        if item not in ['has','was']:\n",
      "            stemmed.append(stemmer.lemmatize(item))\n",
      "    return ' '.join(stemmed)\n",
      "\n",
      "for filename in os.listdir(path):\n",
      "    with open(path+filename) as f:\n",
      "        doc = \"\"\n",
      "        for sent in f:\n",
      "            if sent !='\\n':\n",
      "                doc += stem_tokens(sent.split(' '),stemmer)\n",
      "        doc = doc[:-1]\n",
      "\n",
      "    with open('d:\\\\nlp\\\\brown2\\\\'+filename,'w') as f:\n",
      "        f.write(doc[:-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from collections import Counter\n",
      "import os\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "\n",
      "path = 'd:\\\\nlp\\\\brown2\\\\'\n",
      "documents = []\n",
      "for filename in os.listdir(path):\n",
      "    with open(path+filename) as f1:\n",
      "        documents += f1.read().split('\\n')\n",
      "\n",
      "# remove common words and tokenize\n",
      "stopwords = set(stopwords.raw('english').split()) ## \u83b7\u53d6\u82f1\u6587\u505c\u7528\u8bcd\u8868\n",
      "texts = [[word for word in document.split() if word not in stopwords]\n",
      "         for document in documents]\n",
      "\n",
      "# remove words that appear only once\n",
      "\n",
      "all_tokens = [word for text in texts for word in text]\n",
      "fd = Counter(all_tokens)\n",
      "tokens_once = set(word for word in fd if fd[word] == 1)\n",
      "texts = [[word for word in text if word not in tokens_once]\n",
      "         for text in texts]\n",
      "\n",
      "tokens = set(word for word in c if c[word] > 1)\n",
      "cfd_win = {}\n",
      "cfd_sent = {}\n",
      "pmi_win = {}\n",
      "pmi_sent ={}\n",
      "for i in tokens:\n",
      "    cfd_win[i] = Counter()\n",
      "    cfd_sent[i] = Counter()\n",
      "    pmi_win[i] = {}\n",
      "    pmi_sent[i] = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sent in texts:\n",
      "    for (index,token) in enumerate(sent):\n",
      "        if index > 4:\n",
      "            window = sent[index-5:index+5]   ## \u524d\u540e5\u4e2a\u5355\u8bcd,\u7a97\u53e3\n",
      "        else:\n",
      "            window = sent[0:index+5]         ## \u524d\u540e5\u4e2a\u5355\u8bcd,\u7a97\u53e3\n",
      "        cfd_win[token].update(window)       ## \u589e\u52a0\u5165\u5f53\u524d\u5355\u8bcd\u7684\u76f8\u5173\u5355\u8bcd\n",
      "        cfd_sent[token].update(sent)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "tokens = set(word for word in c if c[word] > 1)\n",
      "pmi_win = {}\n",
      "pmi_sent ={}\n",
      "for i in tokens:\n",
      "    pmi_win[i] = {}\n",
      "    pmi_sent[i] = {}\n",
      "    \n",
      "for t1 in cfd_win:\n",
      "    for t2 in cfd_win[t1]:\n",
      "        cfd = float(cfd_win[t1][t2]+cfd_win[t2][t1])/2.0\n",
      "        pmi_win[t1][t2] = cfd/len(tokens)*math.log(cfd*len(tokens)/(fd[t1]*fd[t2]),2)\n",
      "for t1 in cfd_sent:\n",
      "    for t2 in cfd_sent[t1]:\n",
      "        cfd = float(cfd_sent[t1][t2]+cfd_sent[t2][t1])/2.0\n",
      "        pmi_sent[t1][t2] = cfd/len(tokens)*math.log(cfd*len(tokens)/(fd[t1]*fd[t2]),2)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Counter(pmi_sent['justice'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove common words and tokenize\n",
      "stoplist = set('for a of the and to in'.split())\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "         for document in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}