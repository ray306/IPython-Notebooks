{
 "metadata": {
  "name": "",
  "signature": "sha256:dbd7fc6e82302f6f35a16348d5e3d9967d5ee2148a11e00618e2f5d822ec0118"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "'''\n",
      " Deep Belief Nets (DBN)\n",
      " References :\n",
      "   - Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle: Greedy Layer-Wise\n",
      "   Training of Deep Networks, Advances in Neural Information Processing\n",
      "   Systems 19, 2007\n",
      "   - DeepLearningTutorials\n",
      "   https://github.com/lisa-lab/DeepLearningTutorials\n",
      "'''\n",
      "\n",
      "import sys\n",
      "import numpy\n",
      "from HiddenLayer import HiddenLayer\n",
      "from LogisticRegression import LogisticRegression\n",
      "from RBM import RBM\n",
      "from utils import *\n",
      "\n",
      "\n",
      "class DBN(object):\n",
      "    def __init__(self, input=None, label=None,\\\n",
      "                 n_ins=2, hidden_layer_sizes=[3, 3], n_outs=2,\\\n",
      "                 numpy_rng=None):\n",
      "        \n",
      "        self.x = input\n",
      "        self.y = label\n",
      "\n",
      "        self.sigmoid_layers = []\n",
      "        self.rbm_layers = []\n",
      "        self.n_layers = len(hidden_layer_sizes)  # = len(self.rbm_layers)\n",
      "\n",
      "        if numpy_rng is None:\n",
      "            numpy_rng = numpy.random.RandomState(1234)\n",
      "\n",
      "        \n",
      "        assert self.n_layers > 0\n",
      "\n",
      "\n",
      "        # construct multi-layer\n",
      "        for i in range(self.n_layers):\n",
      "            # layer_size\n",
      "            if i == 0:\n",
      "                input_size = n_ins\n",
      "            else:\n",
      "                input_size = hidden_layer_sizes[i - 1]\n",
      "\n",
      "            # layer_input\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
      "                \n",
      "            # construct sigmoid_layer\n",
      "            sigmoid_layer = HiddenLayer(input=layer_input,\n",
      "                                        n_in=input_size,\n",
      "                                        n_out=hidden_layer_sizes[i],\n",
      "                                        numpy_rng=numpy_rng,\n",
      "                                        activation=sigmoid)\n",
      "            self.sigmoid_layers.append(sigmoid_layer)\n",
      "\n",
      "\n",
      "            # construct rbm_layer\n",
      "            rbm_layer = RBM(input=layer_input,\n",
      "                            n_visible=input_size,\n",
      "                            n_hidden=hidden_layer_sizes[i],\n",
      "                            W=sigmoid_layer.W,     # W, b are shared\n",
      "                            hbias=sigmoid_layer.b)\n",
      "            self.rbm_layers.append(rbm_layer)\n",
      "\n",
      "\n",
      "        # layer for output using Logistic Regression\n",
      "        self.log_layer = LogisticRegression(input=self.sigmoid_layers[-1].sample_h_given_v(),\n",
      "                                            label=self.y,\n",
      "                                            n_in=hidden_layer_sizes[-1],\n",
      "                                            n_out=n_outs)\n",
      "\n",
      "        # finetune cost: the negative log likelihood of the logistic regression layer\n",
      "        self.finetune_cost = self.log_layer.negative_log_likelihood()\n",
      "\n",
      "    def pretrain(self, lr=0.1, k=1, epochs=100):\n",
      "        # pre-train layer-wise\n",
      "        for i in range(self.n_layers):\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[i-1].sample_h_given_v(layer_input)\n",
      "            rbm = self.rbm_layers[i]\n",
      "            \n",
      "            for epoch in range(epochs):\n",
      "                rbm.contrastive_divergence(lr=lr, k=k, input=layer_input)\n",
      "\n",
      "    def finetune(self, lr=0.1, epochs=100):\n",
      "        layer_input = self.sigmoid_layers[-1].sample_h_given_v()\n",
      "\n",
      "        # train log_layer\n",
      "        epoch = 0\n",
      "        done_looping = False\n",
      "        while (epoch < epochs) and (not done_looping):\n",
      "            self.log_layer.train(lr=lr, input=layer_input)\n",
      "            \n",
      "            lr *= 0.95\n",
      "            epoch += 1\n",
      "\n",
      "\n",
      "    def predict(self, x):\n",
      "        layer_input = x\n",
      "        \n",
      "        for i in range(self.n_layers):\n",
      "            sigmoid_layer = self.sigmoid_layers[i]\n",
      "            layer_input = sigmoid_layer.output(input=layer_input)\n",
      "\n",
      "        out = self.log_layer.predict(layer_input)\n",
      "        return out\n",
      "\n",
      "\n",
      "\n",
      "def test_dbn(pretrain_lr=0.1, pretraining_epochs=1000, k=1, \\\n",
      "             finetune_lr=0.1, finetune_epochs=200,hidden=[3, 3]):\n",
      "\n",
      "    x = numpy.array([[1,1,1,0,0,0],\n",
      "                     [1,0,1,0,0,0],\n",
      "                     [1,1,1,0,0,0],\n",
      "                     [0,0,1,1,1,0],\n",
      "                     [0,0,1,1,0,0],\n",
      "                     [0,0,1,1,1,0]])\n",
      "    y = numpy.array([[1, 0],\n",
      "                     [1, 0],\n",
      "                     [1, 0],\n",
      "                     [0, 1],\n",
      "                     [0, 1],\n",
      "                     [0, 1]])\n",
      "\n",
      "    \n",
      "    rng = numpy.random.RandomState(123)\n",
      "\n",
      "    # construct DBN\n",
      "    dbn = DBN(input=x, label=y, n_ins=6, hidden_layer_sizes=hidden, n_outs=2, numpy_rng=rng)\n",
      "\n",
      "    # pre-training (TrainUnsupervisedDBN)\n",
      "    dbn.pretrain(lr=pretrain_lr, k=1, epochs=pretraining_epochs)\n",
      "    \n",
      "    # fine-tuning (DBNSupervisedFineTuning)\n",
      "    dbn.finetune(lr=finetune_lr, epochs=finetune_epochs)\n",
      "\n",
      "\n",
      "    # test\n",
      "    x = numpy.array([[1, 1, 0, 0, 0, 0],\n",
      "                     [0, 0, 0, 1, 1, 0],\n",
      "                     [1, 1, 1, 1, 1, 0],\n",
      "                     [0,0,1,1,1,0]])\n",
      "    \n",
      "    print(dbn.predict(x))\n",
      "\n",
      "test_dbn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 1 1 0 0 0]\n",
        " [1 0 1 0 0 0]\n",
        " [1 1 1 0 0 0]\n",
        " [0 0 1 1 1 0]\n",
        " [0 0 1 1 0 0]\n",
        " [0 0 1 1 1 0]] [[ 0.06548973 -0.07128689 -0.09104952]\n",
        " [ 0.01710492  0.07315632 -0.02563118]\n",
        " [ 0.16025473  0.06160991 -0.00635603]\n",
        " [-0.03596083 -0.05227399  0.0763499 ]\n",
        " [-0.02047592 -0.14677403 -0.03398525]\n",
        " [ 0.0793318  -0.10583609 -0.10818275]]\n",
        "[[1 0 1]\n",
        " [0 1 1]\n",
        " [0 1 0]\n",
        " [1 0 0]\n",
        " [1 1 1]\n",
        " [1 0 1]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[-0.04909913  0.26225944  0.29610668]\n",
        " [ 0.00122445  0.0826353  -0.2562544 ]\n",
        " [-0.12180968 -0.05678253  0.24420611]]\n",
        "[[1 1 1 0 0 0]\n",
        " [1 0 1 0 0 0]\n",
        " [1 1 1 0 0 0]\n",
        " [0 0 1 1 1 0]\n",
        " [0 0 1 1 0 0]\n",
        " [0 0 1 1 1 0]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[-7.40881126  3.10011776  3.4389685 ]\n",
        " [-5.58493803  0.37580065  0.89911014]\n",
        " [ 3.14904227  3.30557682  0.37497678]\n",
        " [ 6.80474504 -2.13167993 -4.49133274]\n",
        " [ 1.79434855 -4.72288461 -2.77621468]\n",
        " [-4.48878088 -3.06232873 -2.82367623]]\n",
        "[[1 0 1]\n",
        " [0 1 1]\n",
        " [0 1 0]\n",
        " [1 0 0]\n",
        " [1 1 1]\n",
        " [1 0 1]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[ 7.26963303 -3.90493087 -5.38714785]\n",
        " [-1.39958425  1.73604526  3.31391491]\n",
        " [-6.12293626  2.387245    3.94831598]]\n",
        "[[1 1 0 0 0 0]\n",
        " [0 0 0 1 1 0]\n",
        " [1 1 1 1 1 0]\n",
        " [0 0 1 1 1 0]] [[-7.40881126  3.10011776  3.4389685 ]\n",
        " [-5.58493803  0.37580065  0.89911014]\n",
        " [ 3.14904227  3.30557682  0.37497678]\n",
        " [ 6.80474504 -2.13167993 -4.49133274]\n",
        " [ 1.79434855 -4.72288461 -2.77621468]\n",
        " [-4.48878088 -3.06232873 -2.82367623]]\n",
        "[[  1.58130474e-06   9.64508273e-01   9.76320279e-01]\n",
        " [  9.99735018e-01   8.85772585e-04   3.75658480e-04]\n",
        " [  1.66710433e-01   4.38643559e-01   4.01791298e-02]\n",
        " [  9.99988631e-01   2.36008824e-02   5.46473776e-04]] [[ 7.26963303 -3.90493087 -5.38714785]\n",
        " [-1.39958425  1.73604526  3.31391491]\n",
        " [-6.12293626  2.387245    3.94831598]]\n",
        "[[ 0.72344411  0.27655589]\n",
        " [ 0.29602118  0.70397882]\n",
        " [ 0.43766986  0.56233014]\n",
        " [ 0.29608224  0.70391776]]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PIL import Image, ImageFont, ImageDraw\n",
      "pretrain_lr=0.1\n",
      "pretraining_epochs=1000\n",
      "k=1\n",
      "finetune_lr=0.1\n",
      "finetune_epochs=200\n",
      "size=50\n",
      "font = ImageFont.truetype(\"simhei.ttf\", int(size/5*4))\n",
      "def getImg(chs=['','','','']):\n",
      "    arr = numpy.zeros((len(chs),size*size))\n",
      "    for n,ch in enumerate(chs):\n",
      "        im = Image.new(\"RGB\", (size,size), (255, 255, 255))\n",
      "        dr = ImageDraw.Draw(im)\n",
      "        dr.text((0,0), ch, font=font, fill=\"#000000\")\n",
      "        t = numpy.array(im.getdata(),numpy.uint8)[:,0].tolist()\n",
      "        iTwo = ['']*size*size\n",
      "        for i in range(size*size):\n",
      "            iTwo[i] = 1 if t[i] < 20 else 0\n",
      "        arr[n] = iTwo\n",
      "    return arr   \n",
      "\n",
      "x = getImg(['\u5999','\u59a8','\u597d','\u5999','\u59a8','\u597d','\u7684','\u5f97','\u70b9','\u5999','\u59a8','\u597d','\u5999','\u59a8','\u597d','\u7684','\u5f97','\u70b9'])\n",
      "y = numpy.array([[1,0], [1,0], [1,0], [1,0], [1,0], [1,0], [0,0], [0,0], [0,0], [1,0], [1,0], [1,0], [1,0], [1,0], [1,0], [0,0], [0,0], [0,0]])\n",
      "\n",
      "\n",
      "rng = numpy.random.RandomState(123)\n",
      "\n",
      "# construct DBN\n",
      "dbn = DBN(input=x, label=y, n_ins=size*size, hidden_layer_sizes=[5, 2], n_outs=2, numpy_rng=rng)\n",
      "\n",
      "# pre-training (TrainUnsupervisedDBN)\n",
      "dbn.pretrain(lr=pretrain_lr, k=1, epochs=pretraining_epochs)\n",
      "\n",
      "# fine-tuning (DBNSupervisedFineTuning)\n",
      "dbn.finetune(lr=finetune_lr, epochs=finetune_epochs)\n",
      "\n",
      "\n",
      "# test\n",
      "x = getImg(['\u5999','\u70b9',' '])\n",
      "\n",
      "print(dbn.predict(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " ..., \n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]] [[  1.57175348e-04  -1.71088532e-04  -2.18518837e-04   4.10518153e-05\n",
        "    1.75575176e-04]\n",
        " [ -6.15148319e-05   3.84611359e-04   1.47863791e-04  -1.52544788e-05\n",
        "   -8.63059854e-05]\n",
        " [ -1.25457587e-04   1.83239766e-04  -4.91422043e-05  -3.52257683e-04\n",
        "   -8.15645957e-05]\n",
        " ..., \n",
        " [  7.10003738e-05   2.16953432e-04   3.43131542e-04   5.05080971e-05\n",
        "    1.42807234e-04]\n",
        " [  1.01493530e-04   1.70227291e-04   2.54453054e-04   1.68838743e-04\n",
        "    7.75803220e-05]\n",
        " [  8.55907409e-05   9.68480985e-05   1.63670842e-04   2.52968703e-04\n",
        "   -3.62019784e-04]]\n",
        "[[0 0 1 0 0]\n",
        " [1 1 0 1 0]\n",
        " [1 0 0 0 0]\n",
        " [0 1 1 1 0]\n",
        " [1 0 1 0 0]\n",
        " [1 0 0 0 0]\n",
        " [0 0 1 1 0]\n",
        " [0 0 1 1 1]\n",
        " [0 1 0 0 0]\n",
        " [1 1 0 0 0]\n",
        " [0 0 1 0 1]\n",
        " [1 0 0 0 0]\n",
        " [1 0 1 1 0]\n",
        " [1 0 0 1 1]\n",
        " [1 1 1 0 1]\n",
        " [0 0 1 0 1]\n",
        " [0 1 1 0 0]\n",
        " [0 1 1 1 1]] [[-0.15713994 -0.01005067]\n",
        " [-0.04426276  0.00789826]\n",
        " [-0.00260238  0.08567298]\n",
        " [ 0.03027859 -0.14426408]\n",
        " [ 0.19003342 -0.06033256]]\n",
        "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " ..., \n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[-0.29867534 -5.10338344 -4.80043397 -0.30003882 -5.49985042]\n",
        " [-0.39851691 -4.89946219 -4.79592186 -0.39895674 -5.45014927]\n",
        " [-0.59857872 -4.99945294 -4.80004593 -0.60054001 -5.17171782]\n",
        " ..., \n",
        " [-0.49839581 -5.19991201 -4.99965198 -0.49858138 -5.70001891]\n",
        " [-0.44854817 -4.6495101  -4.84934102 -0.44896114 -5.44947103]\n",
        " [-0.24934464 -5.24961598 -4.55102397 -0.24920909 -5.5494931 ]]\n",
        "[[0 0 1 0 0]\n",
        " [1 1 0 1 0]\n",
        " [1 0 0 0 0]\n",
        " [0 1 1 1 0]\n",
        " [1 0 1 0 0]\n",
        " [1 0 0 0 0]\n",
        " [0 0 1 1 0]\n",
        " [0 0 1 1 1]\n",
        " [0 1 0 0 0]\n",
        " [1 1 0 0 0]\n",
        " [0 0 1 0 1]\n",
        " [1 0 0 0 0]\n",
        " [1 0 1 1 0]\n",
        " [1 0 0 1 1]\n",
        " [1 1 1 0 1]\n",
        " [0 0 1 0 1]\n",
        " [0 1 1 0 0]\n",
        " [0 1 1 1 1]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [[-3.70665436 -5.73994018]\n",
        " [-7.51637772  1.83821786]\n",
        " [-6.58625471  1.74253815]\n",
        " [-3.49664866 -5.95733555]\n",
        " [ 3.10868702  1.14469559]]\n",
        "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  0.]] [[-0.29867534 -5.10338344 -4.80043397 -0.30003882 -5.49985042]\n",
        " [-0.39851691 -4.89946219 -4.79592186 -0.39895674 -5.45014927]\n",
        " [-0.59857872 -4.99945294 -4.80004593 -0.60054001 -5.17171782]\n",
        " ..., \n",
        " [-0.49839581 -5.19991201 -4.99965198 -0.49858138 -5.70001891]\n",
        " [-0.44854817 -4.6495101  -4.84934102 -0.44896114 -5.44947103]\n",
        " [-0.24934464 -5.24961598 -4.55102397 -0.24920909 -5.5494931 ]]\n",
        "[[  5.60417873e-014   1.00000000e+000   3.60025773e-239   1.03798962e-010\n",
        "    0.00000000e+000]\n",
        " [  3.31335044e-033   0.00000000e+000   0.00000000e+000   1.08272753e-033\n",
        "    1.00000000e+000]\n",
        " [  4.95869606e-001   5.24929197e-001   5.40262759e-001   4.95844761e-001\n",
        "    7.89859857e-001]] [[-3.70665436 -5.73994018]\n",
        " [-7.51637772  1.83821786]\n",
        " [-6.58625471  1.74253815]\n",
        " [-3.49664866 -5.95733555]\n",
        " [ 3.10868702  1.14469559]]\n",
        "[[ 0.84382045  0.15617955]\n",
        " [ 0.83853376  0.16146624]\n",
        " [ 0.70086064  0.29913936]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import io\n",
      "import sqlite3\n",
      "import pandas as pd\n",
      "import pandas.io.sql as sql\n",
      "\n",
      "\n",
      "conn = sqlite3.connect('h:/lab/hanzi.db')\n",
      "conn.text_factory = str\n",
      "\n",
      "result = sql.read_frame('SELECT Unicode,Character,IDS1st, IDS2nd,IDS3rd FROM hanzi',conn)\n",
      "result['IDS'] = result['IDS3rd']\n",
      "result['IDS'][result['IDS'].isnull()] = result['IDS2nd'][result['IDS'].isnull()]\n",
      "result['IDS'][result['IDS'].isnull()] = result['IDS1st'][result['IDS'].isnull()]\n",
      "result.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Unicode</th>\n",
        "      <th>Character</th>\n",
        "      <th>IDS1st</th>\n",
        "      <th>IDS2nd</th>\n",
        "      <th>IDS3rd</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> u4E00</td>\n",
        "      <td> \u4e00</td>\n",
        "      <td> \u4e00</td>\n",
        "      <td> None</td>\n",
        "      <td> None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> u4E01</td>\n",
        "      <td> \u4e01</td>\n",
        "      <td> \u4e01</td>\n",
        "      <td> None</td>\n",
        "      <td> None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> u4E02</td>\n",
        "      <td> \u4e02</td>\n",
        "      <td> \u4e02</td>\n",
        "      <td> None</td>\n",
        "      <td> None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> u4E03</td>\n",
        "      <td> \u4e03</td>\n",
        "      <td> \u4e03</td>\n",
        "      <td> None</td>\n",
        "      <td> None</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> u4E04</td>\n",
        "      <td> \u4e04</td>\n",
        "      <td> \u4e04</td>\n",
        "      <td> None</td>\n",
        "      <td> None</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "  Unicode Character IDS1st IDS2nd IDS3rd\n",
        "0   u4E00         \u4e00      \u4e00   None   None\n",
        "1   u4E01         \u4e01      \u4e01   None   None\n",
        "2   u4E02         \u4e02      \u4e02   None   None\n",
        "3   u4E03         \u4e03      \u4e03   None   None\n",
        "4   u4E04         \u4e04      \u4e04   None   None"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PIL import Image, ImageFont, ImageDraw\n",
      "pretrain_lr=0.1\n",
      "pretraining_epochs=1000\n",
      "k=1\n",
      "finetune_lr=0.1\n",
      "finetune_epochs=200\n",
      "size=50\n",
      "font = ImageFont.truetype(\"simhei.ttf\", int(size/5*4))\n",
      "def getImg(chs=['','','','']):\n",
      "    arr = numpy.zeros((len(chs),size*size))\n",
      "    for n,ch in enumerate(chs):\n",
      "        im = Image.new(\"RGB\", (size,size), (255, 255, 255))\n",
      "        dr = ImageDraw.Draw(im)\n",
      "        dr.text((0,0), ch, font=font, fill=\"#000000\")\n",
      "        t = numpy.array(im.getdata(),numpy.uint8)[:,0].tolist()\n",
      "        iTwo = ['']*size*size\n",
      "        for i in range(size*size):\n",
      "            iTwo[i] = 1 if t[i] < 20 else 0\n",
      "        arr[n] = iTwo\n",
      "    return arr   \n",
      "\n",
      "x = getImg(['\u5999','\u59a8','\u597d','\u5999','\u59a8','\u597d','\u7684','\u5f97','\u70b9','\u5999','\u59a8','\u597d','\u5999','\u59a8','\u597d','\u7684','\u5f97','\u70b9'])\n",
      "y = numpy.array([[1,0], [1,0], [1,0], [1,0], [1,0], [1,0], [0,0], [0,0], [0,0], [1,0], [1,0], [1,0], [1,0], [1,0], [1,0], [0,0], [0,0], [0,0]])\n",
      "\n",
      "\n",
      "rng = numpy.random.RandomState(123)\n",
      "\n",
      "# construct DBN\n",
      "dbn = DBN(input=x, label=y, n_ins=size*size, hidden_layer_sizes=[5, 2], n_outs=2, numpy_rng=rng)\n",
      "\n",
      "# pre-training (TrainUnsupervisedDBN)\n",
      "dbn.pretrain(lr=pretrain_lr, k=1, epochs=pretraining_epochs)\n",
      "\n",
      "# fine-tuning (DBNSupervisedFineTuning)\n",
      "dbn.finetune(lr=finetune_lr, epochs=finetune_epochs)\n",
      "\n",
      "\n",
      "# test\n",
      "x = getImg(['\u5999','\u70b9',' '])\n",
      "\n",
      "print(dbn.predict(x))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class LeNetConvPoolLayer(object):\n",
      "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
      "\n",
      "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n",
      "        \"\"\"\n",
      "        Allocate a LeNetConvPoolLayer with shared variable internal parameters.\n",
      "\n",
      "        :type rng: numpy.random.RandomState\n",
      "        :param rng: a random number generator used to initialize weights\n",
      "\n",
      "        :type input: theano.tensor.dtensor4\n",
      "        :param input: symbolic image tensor, of shape image_shape\n",
      "\n",
      "        :type filter_shape: tuple or list of length 4\n",
      "        :param filter_shape: (number of filters, num input feature maps,\n",
      "                              filter height, filter width)\n",
      "\n",
      "        :type image_shape: tuple or list of length 4\n",
      "        :param image_shape: (batch size, num input feature maps,\n",
      "                             image height, image width)\n",
      "\n",
      "        :type poolsize: tuple or list of length 2\n",
      "        :param poolsize: the downsampling (pooling) factor (#rows, #cols)\n",
      "        \"\"\"\n",
      "\n",
      "        assert image_shape[1] == filter_shape[1]\n",
      "        self.input = input\n",
      "\n",
      "        # there are \"num input feature maps * filter height * filter width\"\n",
      "        # inputs to each hidden unit\n",
      "        fan_in = numpy.prod(filter_shape[1:])\n",
      "        # each unit in the lower layer receives a gradient from:\n",
      "        # \"num output feature maps * filter height * filter width\" /\n",
      "        #   pooling size\n",
      "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /\n",
      "                   numpy.prod(poolsize))\n",
      "        # initialize weights with random weights\n",
      "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
      "        self.W = theano.shared(\n",
      "            numpy.asarray(\n",
      "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
      "                dtype=theano.config.floatX\n",
      "            ),\n",
      "            borrow=True\n",
      "        )\n",
      "\n",
      "        # the bias is a 1D tensor -- one bias per output feature map\n",
      "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
      "        self.b = theano.shared(value=b_values, borrow=True)\n",
      "\n",
      "        # convolve input feature maps with filters\n",
      "        conv_out = conv.conv2d(\n",
      "            input=input,\n",
      "            filters=self.W,\n",
      "            filter_shape=filter_shape,\n",
      "            image_shape=image_shape\n",
      "        )\n",
      "\n",
      "        # downsample each feature map individually, using maxpooling\n",
      "        pooled_out = downsample.max_pool_2d(\n",
      "            input=conv_out,\n",
      "            ds=poolsize,\n",
      "            ignore_border=True\n",
      "        )\n",
      "\n",
      "        # add the bias term. Since the bias is a vector (1D array), we first\n",
      "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
      "        # thus be broadcasted across mini-batches and feature map\n",
      "        # width & height\n",
      "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
      "\n",
      "        # store parameters of this layer\n",
      "        self.params = [self.W, self.b]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "from theano import tensor as T\n",
      "from theano.tensor.nnet import conv\n",
      "\n",
      "import numpy\n",
      "x = T.matrix('x')   # the data is presented as rasterized images\n",
      "y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                    # [int] labels\n",
      "\n",
      "######################\n",
      "# BUILD ACTUAL MODEL #\n",
      "######################\n",
      "\n",
      "# Reshape matrix of rasterized images of shape (batch_size, 28 * 28)\n",
      "# to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
      "# (28, 28) is the size of MNIST images.\n",
      "layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
      "\n",
      "# Construct the first convolutional pooling layer:\n",
      "# filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
      "# maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
      "# 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
      "layer0 = LeNetConvPoolLayer(\n",
      "    rng,\n",
      "    input=layer0_input,\n",
      "    image_shape=(batch_size, 1, 28, 28),\n",
      "    filter_shape=(nkerns[0], 1, 5, 5),\n",
      "    poolsize=(2, 2)\n",
      ")\n",
      "\n",
      "# Construct the second convolutional pooling layer\n",
      "# filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
      "# maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
      "# 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
      "layer1 = LeNetConvPoolLayer(\n",
      "    rng,\n",
      "    input=layer0.output,\n",
      "    image_shape=(batch_size, nkerns[0], 12, 12),\n",
      "    filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
      "    poolsize=(2, 2)\n",
      ")\n",
      "\n",
      "# the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
      "# shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
      "# This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
      "# or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
      "layer2_input = layer1.output.flatten(2)\n",
      "\n",
      "# construct a fully-connected sigmoidal layer\n",
      "layer2 = HiddenLayer(\n",
      "    rng,\n",
      "    input=layer2_input,\n",
      "    n_in=nkerns[1] * 4 * 4,\n",
      "    n_out=500,\n",
      "    activation=T.tanh\n",
      ")\n",
      "\n",
      "# classify the values of the fully-connected sigmoidal layer\n",
      "layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
      "\n",
      "# the cost we minimize during training is the NLL of the model\n",
      "cost = layer3.negative_log_likelihood(y)\n",
      "\n",
      "# create a function to compute the mistakes that are made by the model\n",
      "test_model = theano.function(\n",
      "    [index],\n",
      "    layer3.errors(y),\n",
      "    givens={\n",
      "        x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "        y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "validate_model = theano.function(\n",
      "    [index],\n",
      "    layer3.errors(y),\n",
      "    givens={\n",
      "        x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "        y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "# create a list of all model parameters to be fit by gradient descent\n",
      "params = layer3.params + layer2.params + layer1.params + layer0.params\n",
      "\n",
      "# create a list of gradients for all model parameters\n",
      "grads = T.grad(cost, params)\n",
      "\n",
      "# train_model is a function that updates the model parameters by\n",
      "# SGD Since this model has many parameters, it would be tedious to\n",
      "# manually create an update rule for each model parameter. We thus\n",
      "# create the updates list by automatically looping over all\n",
      "# (params[i], grads[i]) pairs.\n",
      "updates = [\n",
      "    (param_i, param_i - learning_rate * grad_i)\n",
      "    for param_i, grad_i in zip(params, grads)\n",
      "]\n",
      "\n",
      "train_model = theano.function(\n",
      "    [index],\n",
      "    cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
      "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'batch_size' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-355b86d0b769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# to a 4D tensor, compatible with our LeNetConvPoolLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# (28, 28) is the size of MNIST images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlayer0_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Construct the first convolutional pooling layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "from theano import tensor as T\n",
      "from theano.tensor.nnet import conv\n",
      "\n",
      "import numpy\n",
      "\n",
      "rng = numpy.random.RandomState(23455)\n",
      "\n",
      "# instantiate 4D tensor for input\n",
      "input = T.tensor4(name='input')\n",
      "\n",
      "# initialize shared variable for weights.\n",
      "w_shp = (2, 3, 9, 9)\n",
      "w_bound = numpy.sqrt(3 * 9 * 9)\n",
      "W = theano.shared( numpy.asarray(\n",
      "            rng.uniform(\n",
      "                low=-1.0 / w_bound,\n",
      "                high=1.0 / w_bound,\n",
      "                size=w_shp),\n",
      "            dtype=input.dtype), name ='W')\n",
      "\n",
      "# initialize shared variable for bias (1D tensor) with random values\n",
      "# IMPORTANT: biases are usually initialized to zero. However in this\n",
      "# particular application, we simply apply the convolutional layer to\n",
      "# an image without learning the parameters. We therefore initialize\n",
      "# them to random values to \"simulate\" learning.\n",
      "b_shp = (2,)\n",
      "b = theano.shared(numpy.asarray(\n",
      "            rng.uniform(low=-.5, high=.5, size=b_shp),\n",
      "            dtype=input.dtype), name ='b')\n",
      "\n",
      "# build symbolic expression that computes the convolution of input with filters in w\n",
      "conv_out = conv.conv2d(input, W)\n",
      "\n",
      "# build symbolic expression to add bias and apply activation function, i.e. produce neural net layer output\n",
      "# A few words on ``dimshuffle`` :\n",
      "#   ``dimshuffle`` is a powerful tool in reshaping a tensor;\n",
      "#   what it allows you to do is to shuffle dimension around\n",
      "#   but also to insert new ones along which the tensor will be\n",
      "#   broadcastable;\n",
      "#   dimshuffle('x', 2, 'x', 0, 1)\n",
      "#   This will work on 3d tensors with no broadcastable\n",
      "#   dimensions. The first dimension will be broadcastable,\n",
      "#   then we will have the third dimension of the input tensor as\n",
      "#   the second of the resulting tensor, etc. If the tensor has\n",
      "#   shape (20, 30, 40), the resulting tensor will have dimensions\n",
      "#   (1, 40, 1, 20, 30). (AxBxC tensor is mapped to 1xCx1xAxB tensor)\n",
      "#   More examples:\n",
      "#    dimshuffle('x') -> make a 0d (scalar) into a 1d vector\n",
      "#    dimshuffle(0, 1) -> identity\n",
      "#    dimshuffle(1, 0) -> inverts the first and second dimensions\n",
      "#    dimshuffle('x', 0) -> make a row out of a 1d vector (N to 1xN)\n",
      "#    dimshuffle(0, 'x') -> make a column out of a 1d vector (N to Nx1)\n",
      "#    dimshuffle(2, 0, 1) -> AxBxC to CxAxB\n",
      "#    dimshuffle(0, 'x', 1) -> AxB to Ax1xB\n",
      "#    dimshuffle(1, 'x', 0) -> AxB to Bx1xA\n",
      "output = T.nnet.sigmoid(conv_out + b.dimshuffle('x', 0, 'x', 'x'))\n",
      "\n",
      "# create theano function to compute filtered images\n",
      "f = theano.function([input], output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import pylab\n",
      "from PIL import Image\n",
      "\n",
      "# open random image of dimensions 639x516\n",
      "img = Image.open(open('doc/images/3wolfmoon.jpg'))\n",
      "# dimensions are (height, width, channel)\n",
      "img = numpy.asarray(img, dtype='float64') / 256.\n",
      "\n",
      "# put image in 4D tensor of shape (1, 3, height, width)\n",
      "img_ = img.transpose(2, 0, 1).reshape(1, 3, 639, 516)\n",
      "filtered_img = f(img_)\n",
      "\n",
      "# plot original image and first and second components of output\n",
      "pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(img)\n",
      "pylab.gray();\n",
      "# recall that the convOp output (filtered image) is actually a \"minibatch\",\n",
      "# of size 1 here, so we take index 0 in the first dimension:\n",
      "pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(filtered_img[0, 0, :, :])\n",
      "pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(filtered_img[0, 1, :, :])\n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "FileNotFoundError",
       "evalue": "[Errno 2] No such file or directory: 'doc/images/3wolfmoon.jpg'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-26-bcac127a19b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# open random image of dimensions 639x516\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doc/images/3wolfmoon.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# dimensions are (height, width, channel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m256.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'doc/images/3wolfmoon.jpg'"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from keras.datasets import cifar10\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "from keras.models import Sequential\n",
      "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
      "from keras.optimizers import SGD, Adadelta, Adagrad\n",
      "from keras.utils import np_utils, generic_utils\n",
      "\n",
      "'''\n",
      "    Train a (fairly simple) deep CNN on the CIFAR10 small images dataset.\n",
      "    GPU run command:\n",
      "        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_cnn.py\n",
      "    It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
      "    (it's still underfitting at that point, though).\n",
      "'''\n",
      "\n",
      "batch_size = 32\n",
      "nb_classes = 10\n",
      "nb_epoch = 200\n",
      "data_augmentation = True\n",
      "\n",
      "# the data, shuffled and split between tran and test sets\n",
      "(X_train, y_train), (X_test, y_test) = cifar10.load_data(test_split=0.1)\n",
      "print(X_train.shape[0], 'train samples')\n",
      "print(X_test.shape[0], 'test samples')\n",
      "\n",
      "# convert class vectors to binary class matrices\n",
      "Y_train = np_utils.to_categorical(y_train)\n",
      "Y_test = np_utils.to_categorical(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "45000 train samples\n",
        "5000 test samples\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
        "       ..., \n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
      "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
      "from keras.optimizers import SGD\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Convolution2D(2, 3, 3, 3, border_mode='full')) \n",
      "model.add(Activation('relu'))\n",
      "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
      "model.add(Dropout(0.25))\n",
      "\n",
      "model.add(Flatten(4*8*8))\n",
      "model.add(Dense(4*8*8, 256))\n",
      "model.add(Activation('relu'))\n",
      "model.add(Dropout(0.5))\n",
      "\n",
      "model.add(Dense(2, 10))\n",
      "model.add(Activation('softmax'))\n",
      "\n",
      "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
      "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
      "\n",
      "# X_train = getImg(['\u5999','\u59a8','\u597d','\u7684','\u5f97','\u70b9'])\n",
      "# Y_train = np.array([[1],\n",
      "#                  [1],\n",
      "#                  [1],\n",
      "#                  [1],\n",
      "#                  [1],\n",
      "#                  [1],\n",
      "#                  [0],\n",
      "#                  [0],\n",
      "#                  [0]])\n",
      "\n",
      "# X_test = getImg(['\u5999','\u8981'])\n",
      "# Y_test = np.array([[1],[0]])\n",
      "# (X_train, y_train), (X_test, y_test) = cifar10.load_data(test_split=0.1)\n",
      "# print(X_train.shape[0], 'train samples')\n",
      "# print(X_test.shape[0], 'test samples')\n",
      "\n",
      "# X_train=X_train[:10]\n",
      "# y_train=y_train[:10]\n",
      "# X_test=X_test[:10]\n",
      "# y_test=y_test[:10]\n",
      "\n",
      "datagen = ImageDataGenerator(\n",
      "        featurewise_center=True, # set input mean to 0 over the dataset\n",
      "        samplewise_center=False, # set each sample mean to 0\n",
      "        featurewise_std_normalization=True, # divide inputs by std of the dataset\n",
      "        samplewise_std_normalization=False, # divide each input by its std\n",
      "        zca_whitening=False, # apply ZCA whitening\n",
      "        rotation_range=20, # randomly rotate images in the range (degrees, 0 to 180)\n",
      "        width_shift_range=0.2, # randomly shift images horizontally (fraction of total width)\n",
      "        height_shift_range=0.2, # randomly shift images vertically (fraction of total height)\n",
      "        horizontal_flip=True, # randomly flip images\n",
      "        vertical_flip=False) # randomly flip images\n",
      "\n",
      "# compute quantities required for featurewise normalization \n",
      "# (std, mean, and principal components if ZCA whitening is applied)\n",
      "datagen.fit(X_train)\n",
      "nb_epoch = 200\n",
      "for e in range(nb_epoch):\n",
      "    print('-'*40)\n",
      "    print('Epoch', e)\n",
      "    print('-'*40)\n",
      "    print(\"Training...\")\n",
      "    # batch train with realtime data augmentation\n",
      "    progbar = generic_utils.Progbar(X_train.shape[0])\n",
      "    for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n",
      "        loss = model.train(X_batch, Y_batch)\n",
      "        progbar.add(X_batch.shape[0], values=[(\"train loss\", loss)])\n",
      "\n",
      "    print(\"Testing...\")\n",
      "    # test time!\n",
      "    progbar = generic_utils.Progbar(X_test.shape[0])\n",
      "    for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n",
      "        score = model.test(X_batch, Y_batch)\n",
      "        progbar.add(X_batch.shape[0], values=[(\"test loss\", score)])\n",
      "model.fit(X_train, Y_train, batch_size=1, nb_epoch=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------------------------------\n",
        "Epoch 0\n",
        "----------------------------------------\n",
        "Training...\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "total size of new array must be unchanged\nApply node that caused the error: Reshape{2}(Elemwise{mul,no_inplace}.0, MakeVector.0)\nInputs types: [TensorType(float64, 4D), TensorType(int64, vector)]\nInputs shapes: [(32, 2, 17, 17), (2,)]\nInputs strides: [(4624, 2312, 136, 8), (8,)]\nInputs values: ['not shown', array([ 32, 256], dtype=int64)]\n\nBacktrace when the node is created:\n  File \"H:\\Anaconda3\\lib\\site-packages\\theano\\gof\\type.py\", line 318, in __call__\n    return utils.add_tag_trace(self.make_variable(name))\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-59-2e495a9a0777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mprogbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\theano\\gof\\link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exc_value = exc_type(str(exc_value) + detailed_err_msg +\n\u001b[0;32m    205\u001b[0m                          '\\n' + '\\n'.join(hints))\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mH:\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: total size of new array must be unchanged\nApply node that caused the error: Reshape{2}(Elemwise{mul,no_inplace}.0, MakeVector.0)\nInputs types: [TensorType(float64, 4D), TensorType(int64, vector)]\nInputs shapes: [(32, 2, 17, 17), (2,)]\nInputs strides: [(4624, 2312, 136, 8), (8,)]\nInputs values: ['not shown', array([ 32, 256], dtype=int64)]\n\nBacktrace when the node is created:\n  File \"H:\\Anaconda3\\lib\\site-packages\\theano\\gof\\type.py\", line 318, in __call__\n    return utils.add_tag_trace(self.make_variable(name))\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
       ]
      }
     ],
     "prompt_number": 59
    }
   ],
   "metadata": {}
  }
 ]
}